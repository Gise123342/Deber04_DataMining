{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb0e0fa3-1f0b-4a31-8bee-c92663ac927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d956ad9c-b7cc-47d1-b07c-03f1994cfb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark iniciado correctamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaNYCTaxiRaw\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\" Spark iniciado correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6427fd-8891-4abf-8ba2-f5b237348b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Postgres ‚Üí esquema: raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pg_url = f\"jdbc:postgresql://{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DB')}\"\n",
    "pg_props = {\n",
    "    \"user\": os.getenv(\"PG_USER\"),\n",
    "    \"password\": os.getenv(\"PG_PASSWORD\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "schema_raw = os.getenv(\"PG_SCHEMA_RAW\")\n",
    "print(f\"Conectando a Postgres ‚Üí esquema: {schema_raw}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cc80a1-f06a-49fa-b567-83a8b2950312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def load_parquet_to_postgres_with_log(parquet_path, table_name):\n",
    "    \"\"\"\n",
    "    Carga un archivo Parquet a una tabla Postgres dentro del esquema RAW.\n",
    "    Registra conteo de filas y timestamp UTC de ingesta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_raw = os.getenv(\"PG_SCHEMA_RAW\")\n",
    "        print(f\"Iniciando carga de: {parquet_path} ‚Üí {schema_raw}.{table_name}\")\n",
    "        start_time = datetime.utcnow()\n",
    "\n",
    "        # Leer archivo Parquet\n",
    "        df = spark.read.parquet(parquet_path)\n",
    "\n",
    "        # Agregar columna de metadatos\n",
    "        df = df.withColumn(\"ingested_at_utc\", F.lit(start_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "        # Escribir en Postgres\n",
    "        df.write.jdbc(\n",
    "            url=pg_url,\n",
    "            table=f\"{schema_raw}.{table_name}\",\n",
    "            mode=\"append\",\n",
    "            properties=pg_props\n",
    "        )\n",
    "\n",
    "        # Log de filas\n",
    "        row_count = df.count()\n",
    "        print(f\"{table_name}: {row_count} filas insertadas. ({start_time})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando {table_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799475fa-da8a-4f4e-ae7b-1405a100122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨á Descargando yellow_tripdata_2019-01.parquet ...\n",
      " Guardado en /data/yellow_tripdata_2019-01.parquet\n",
      "‚¨á Descargando yellow_tripdata_2019-02.parquet ...\n",
      " Guardado en /data/yellow_tripdata_2019-02.parquet\n",
      "‚¨á Descargando green_tripdata_2019-01.parquet ...\n",
      " Guardado en /data/green_tripdata_2019-01.parquet\n",
      "‚¨á Descargando green_tripdata_2019-02.parquet ...\n",
      " Guardado en /data/green_tripdata_2019-02.parquet\n",
      "‚¨á Descargando taxi+_zone_lookup.parquet ...\n",
      "Error 403: https://d37ci6vzurychx.cloudfront.net/trip-data/taxi+_zone_lookup.parquet\n",
      "\n",
      "Descarga finalizada (2 meses de prueba).\n",
      "Para todo el rango 2015‚Äì2025, cambia las l√≠neas:\n",
      "years = range(2015, 2026)\n",
      "months = range(1, 13)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Carpeta dentro del contenedor (vinculada a ./data del host)\n",
    "data_dir = \"/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# URL base del dataset oficial del NYC TLC (formato Parquet)\n",
    "#  Todos los enlaces de la p√°gina oficial redirigen a este mismo dominio.\n",
    "base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "\n",
    "# Rango peque√±o de prueba (solo 2 meses, un a√±o)\n",
    "# Para el deber completo: usar years = range(2015, 2026) y months = range(1, 13)\n",
    "years = [2019]\n",
    "months = [1, 2]\n",
    "services = [\"yellow\", \"green\"]\n",
    "\n",
    "def download_file(url, local_path):\n",
    "    \"\"\"Descarga el archivo desde la URL si no existe localmente.\"\"\"\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"‚¨á Descargando {os.path.basename(local_path)} ...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\" Guardado en {local_path}\")\n",
    "        else:\n",
    "            print(f\"Error {r.status_code}: {url}\")\n",
    "    else:\n",
    "        print(f\"Ya existe: {os.path.basename(local_path)}\")\n",
    "\n",
    "# Descarga iterativa de ambos servicios y meses definidos\n",
    "for service in services:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            file_name = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "            url = base_url + file_name\n",
    "            local_path = os.path.join(data_dir, file_name)\n",
    "            download_file(url, local_path)\n",
    "\n",
    "#Archivo de zonas (solo uno, requerido para la OBT)\n",
    "zones_url = base_url + \"taxi+_zone_lookup.parquet\"\n",
    "zones_path = os.path.join(data_dir, \"taxi+_zone_lookup.parquet\")\n",
    "download_file(zones_url, zones_path)\n",
    "\n",
    "print(\"\\nDescarga finalizada (2 meses de prueba).\")\n",
    "print(\"Para todo el rango 2015‚Äì2025, cambia las l√≠neas:\")\n",
    "print(\"years = range(2015, 2026)\")\n",
    "print(\"months = range(1, 13)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976d915d-8156-40a9-b284-831312d91a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                NULL|       NULL|\n",
      "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                NULL|       NULL|\n",
      "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|            3.0|          0.0|       1.0|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                NULL|       NULL|\n",
      "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|            5.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                NULL|       NULL|\n",
      "|       2| 2018-11-28 15:56:57|  2018-11-28 15:58:33|            5.0|          0.0|       2.0|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                NULL|       NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/data/yellow_tripdata_2019-01.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5055cb95-f379-421f-8d08-c1389c837948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando taxi_zone_lookup.csv ...\n",
      "CSV guardado en /data/taxi_zone_lookup.csv\n",
      "Archivo convertido a Parquet en /data/taxi_zone_lookup.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ruta y URL oficial actualizada\n",
    "data_dir = \"/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "csv_path = os.path.join(data_dir, \"taxi_zone_lookup.csv\")\n",
    "parquet_path = os.path.join(data_dir, \"taxi_zone_lookup.parquet\")\n",
    "\n",
    "# Descargar CSV\n",
    "print(\"Descargando taxi_zone_lookup.csv ...\")\n",
    "r = requests.get(zones_url, stream=True)\n",
    "if r.status_code == 200:\n",
    "    with open(csv_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"CSV guardado en {csv_path}\")\n",
    "else:\n",
    "    print(f\"Error {r.status_code} descargando el archivo.\")\n",
    "\n",
    "# Convertir CSV a Parquet\n",
    "if os.path.exists(csv_path):\n",
    "    df_pd = pd.read_csv(csv_path)\n",
    "    spark_df = spark.createDataFrame(df_pd)\n",
    "    spark_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    print(f\"Archivo convertido a Parquet en {parquet_path}\")\n",
    "else:\n",
    "    print(\"No se encontr√≥ el CSV para convertirlo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671f1ad1-7c3d-4bd9-a164-9dbef09ee2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquemas 'raw' y 'analytics' creados/verificados correctamente.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import os\n",
    "\n",
    "# Crear conexi√≥n\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"PG_HOST\"),\n",
    "    port=os.getenv(\"PG_PORT\"),\n",
    "    database=os.getenv(\"PG_DB\"),\n",
    "    user=os.getenv(\"PG_USER\"),\n",
    "    password=os.getenv(\"PG_PASSWORD\")\n",
    ")\n",
    "\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Crear esquemas raw y analytics si no existen\n",
    "cur.execute(\"CREATE SCHEMA IF NOT EXISTS raw;\")\n",
    "cur.execute(\"CREATE SCHEMA IF NOT EXISTS analytics;\")\n",
    "\n",
    "print(\"Esquemas 'raw' y 'analytics' creados/verificados correctamente.\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c6789e-8292-470f-93b5-48a5a0469a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas eliminadas correctamente (yellow, green, lookup).\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Conexi√≥n\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"PG_HOST\"),\n",
    "    port=os.getenv(\"PG_PORT\"),\n",
    "    database=os.getenv(\"PG_DB\"),\n",
    "    user=os.getenv(\"PG_USER\"),\n",
    "    password=os.getenv(\"PG_PASSWORD\")\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Eliminar tablas si existen (o vaciarlas)\n",
    "cur.execute(\"DROP TABLE IF EXISTS raw.yellow_taxi_trip CASCADE;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS raw.green_taxi_trip CASCADE;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS raw.taxi_zone_lookup CASCADE;\")\n",
    "\n",
    "print(\"Tablas eliminadas correctamente (yellow, green, lookup).\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065eba53-7191-476c-9e4d-91be49d11f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando carga masiva hacia esquema 'raw'...\n",
      "\n",
      " Encontrado: yellow_tripdata_2019-01.parquet\n",
      "Iniciando carga de: /data/yellow_tripdata_2019-01.parquet ‚Üí raw.yellow_taxi_trip\n",
      "yellow_taxi_trip: 7696617 filas insertadas. (2025-11-11 18:43:23.411340)\n",
      " Encontrado: yellow_tripdata_2019-02.parquet\n",
      "Iniciando carga de: /data/yellow_tripdata_2019-02.parquet ‚Üí raw.yellow_taxi_trip\n",
      "yellow_taxi_trip: 7049370 filas insertadas. (2025-11-11 18:44:53.487189)\n",
      " Encontrado: green_tripdata_2019-01.parquet\n",
      "Iniciando carga de: /data/green_tripdata_2019-01.parquet ‚Üí raw.green_taxi_trip\n",
      "green_taxi_trip: 672105 filas insertadas. (2025-11-11 18:46:19.508169)\n",
      " Encontrado: green_tripdata_2019-02.parquet\n",
      "Iniciando carga de: /data/green_tripdata_2019-02.parquet ‚Üí raw.green_taxi_trip\n",
      "green_taxi_trip: 615594 filas insertadas. (2025-11-11 18:46:27.449328)\n",
      "Iniciando carga de: /data/taxi_zone_lookup.parquet ‚Üí raw.taxi_zone_lookup\n",
      "taxi_zone_lookup: 265 filas insertadas. (2025-11-11 18:46:34.879054)\n",
      "\n",
      "Carga masiva finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Puedes ajustar estos rangos f√°cilmente \n",
    "years = [2019]          # o range(2015, 2026)\n",
    "months = range(1, 3)    # meses del 1 al 2 (enero y febrero)\n",
    "services = [\"yellow\", \"green\"]  # datasets a incluir\n",
    "\n",
    "data_dir = \"/data\"\n",
    "schema_raw = os.getenv(\"PG_SCHEMA_RAW\")\n",
    "\n",
    "def bulk_load_parquets_to_postgres():\n",
    "    print(f\"\\nIniciando carga masiva hacia esquema '{schema_raw}'...\\n\")\n",
    "\n",
    "    # Cargar taxis (Yellow y Green)\n",
    "    for service in services:\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                file_name = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "                parquet_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "                if os.path.exists(parquet_path):\n",
    "                    print(f\" Encontrado: {file_name}\")\n",
    "                    load_parquet_to_postgres_with_log(parquet_path, f\"{service}_taxi_trip\")\n",
    "                else:\n",
    "                    print(f\"No encontrado: {file_name}\")\n",
    "\n",
    "    # Cargar tabla de zonas (si existe)\n",
    "    lookup_path = os.path.join(data_dir, \"taxi_zone_lookup.parquet\")\n",
    "    if os.path.exists(lookup_path):\n",
    "        load_parquet_to_postgres_with_log(lookup_path, \"taxi_zone_lookup\")\n",
    "    else:\n",
    "        print(\"taxi_zone_lookup.parquet no encontrado en /data\")\n",
    "\n",
    "    print(\"\\nCarga masiva finalizada.\\n\")\n",
    "\n",
    "bulk_load_parquets_to_postgres()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546e786c-62d1-41bf-ab38-d389737b71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando tabla: yellow_taxi_trip\n",
      "yellow_taxi_trip contiene 14,745,987 filas.\n",
      "No se pudo mostrar muestra con Spark ([Errno 111] Connection refused) ‚Äî el conteo SQL sigue siendo v√°lido.\n",
      "\n",
      "Verificando tabla: green_taxi_trip\n",
      "green_taxi_trip contiene 1,287,699 filas.\n",
      "No se pudo mostrar muestra con Spark ([Errno 111] Connection refused) ‚Äî el conteo SQL sigue siendo v√°lido.\n",
      "\n",
      "Verificando tabla: taxi_zone_lookup\n",
      "taxi_zone_lookup contiene 265 filas.\n",
      "No se pudo mostrar muestra con Spark ([Errno 111] Connection refused) ‚Äî el conteo SQL sigue siendo v√°lido.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def verify_postgres_table_light(table_name):\n",
    "    \"\"\"Verifica que la tabla existe en Postgres, mostrando conteo SQL y opcionalmente filas de muestra.\"\"\"\n",
    "    print(f\"\\nVerificando tabla: {table_name}\")\n",
    "\n",
    "    # Consultar el conteo directamente desde Postgres\n",
    "    import psycopg2\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.getenv(\"PG_HOST\"),\n",
    "        port=os.getenv(\"PG_PORT\"),\n",
    "        database=os.getenv(\"PG_DB\"),\n",
    "        user=os.getenv(\"PG_USER\"),\n",
    "        password=os.getenv(\"PG_PASSWORD\")\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {schema_raw}.{table_name};\")\n",
    "    count = cur.fetchone()[0]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"{table_name} contiene {count:,} filas.\")\n",
    "\n",
    "    # Mostrar 5 filas con Spark solo si hay memoria disponible\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read.jdbc(\n",
    "                url=pg_url,\n",
    "                table=f\"{schema_raw}.{table_name}\",\n",
    "                properties=pg_props\n",
    "            )\n",
    "            .limit(5)\n",
    "        )\n",
    "        print(f\"Muestra de datos ({table_name}):\")\n",
    "        df.show(5, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo mostrar muestra con Spark ({e}) ‚Äî el conteo SQL sigue siendo v√°lido.\")\n",
    "\n",
    "verify_postgres_table_light(\"yellow_taxi_trip\")\n",
    "verify_postgres_table_light(\"green_taxi_trip\")\n",
    "verify_postgres_table_light(\"taxi_zone_lookup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231bcd82-6e4e-40e9-a788-632d245562d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla raw.ingestion_log creada correctamente.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, os\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"PG_HOST\"),\n",
    "    port=os.getenv(\"PG_PORT\"),\n",
    "    database=os.getenv(\"PG_DB\"),\n",
    "    user=os.getenv(\"PG_USER\"),\n",
    "    password=os.getenv(\"PG_PASSWORD\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS raw.ingestion_log (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    dataset_name VARCHAR(50),\n",
    "    year INTEGER,\n",
    "    month INTEGER,\n",
    "    row_count BIGINT,\n",
    "    ingested_at_utc TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Tabla raw.ingestion_log creada correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241ae178-203b-44a0-90dd-abcd087b7b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Auditor√≠a registrada correctamente en raw.ingestion_log.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, os\n",
    "from datetime import datetime\n",
    "\n",
    "# Registros de auditor√≠a para las cargas ya realizadas\n",
    "ingestas = [\n",
    "    (\"yellow_taxi_trip\", 2019, 1, 7696617),\n",
    "    (\"yellow_taxi_trip\", 2019, 2, 7049370),\n",
    "    (\"green_taxi_trip\", 2019, 1, 672105),\n",
    "    (\"green_taxi_trip\", 2019, 2, 615594),\n",
    "    (\"taxi_zone_lookup\", 2019, None, 265)\n",
    "]\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"PG_HOST\"),\n",
    "    port=os.getenv(\"PG_PORT\"),\n",
    "    database=os.getenv(\"PG_DB\"),\n",
    "    user=os.getenv(\"PG_USER\"),\n",
    "    password=os.getenv(\"PG_PASSWORD\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "for dataset_name, year, month, row_count in ingestas:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO raw.ingestion_log (dataset_name, year, month, row_count, ingested_at_utc)\n",
    "        VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP);\n",
    "    \"\"\", (dataset_name, year, month, row_count))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Auditor√≠a registrada correctamente en raw.ingestion_log.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2b69c9-7241-4332-bd33-059375b335cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>row_count</th>\n",
       "      <th>ingested_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>yellow_taxi_trip</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7696617</td>\n",
       "      <td>2025-11-11 21:42:12.298545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>yellow_taxi_trip</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7049370</td>\n",
       "      <td>2025-11-11 21:42:12.298545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>green_taxi_trip</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>672105</td>\n",
       "      <td>2025-11-11 21:42:12.298545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>green_taxi_trip</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>615594</td>\n",
       "      <td>2025-11-11 21:42:12.298545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>taxi_zone_lookup</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265</td>\n",
       "      <td>2025-11-11 21:42:12.298545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      dataset_name  year  month  row_count            ingested_at_utc\n",
       "0   1  yellow_taxi_trip  2019    1.0    7696617 2025-11-11 21:42:12.298545\n",
       "1   2  yellow_taxi_trip  2019    2.0    7049370 2025-11-11 21:42:12.298545\n",
       "2   3   green_taxi_trip  2019    1.0     672105 2025-11-11 21:42:12.298545\n",
       "3   4   green_taxi_trip  2019    2.0     615594 2025-11-11 21:42:12.298545\n",
       "4   5  taxi_zone_lookup  2019    NaN        265 2025-11-11 21:42:12.298545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd, os\n",
    "\n",
    "# Crear motor SQLAlchemy con las variables del entorno\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DB')}\"\n",
    ")\n",
    "\n",
    "# Leer la tabla de auditor√≠a\n",
    "df_log = pd.read_sql(\"SELECT * FROM raw.ingestion_log ORDER BY id;\", engine)\n",
    "df_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42d547-80b3-4f39-8f9b-ded2d2c66322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
